import pandas as pd
import pyodbc
import requests
from io import BytesIO
import os
import logging
import time
import urllib3
import random
from datetime import datetime

# Deshabilitar warnings de SSL (temporal para pruebas)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def sync_sharepoint_to_sql():
    logging.info("üöÄ Iniciando ACTUALIZACI√ìN OPTIMIZADA SharePoint -> Azure SQL")
    
    # ===== CONFIGURACI√ìN =====
    SHAREPOINT_USERNAME = os.environ['SHAREPOINT_USERNAME']
    SHAREPOINT_PASSWORD = os.environ['SHAREPOINT_PASSWORD']
    SQL_SERVER = os.environ['SQL_SERVER']
    SQL_DATABASE = os.environ['SQL_DATABASE']
    SQL_USERNAME = os.environ['SQL_USERNAME']
    SQL_PASSWORD = os.environ['SQL_PASSWORD']
    
    # ===== CONFIGURACI√ìN POR VENDEDORA =====
    VENDEDORAS_CONFIG = [
        {
            "path": "Documentos compartidos/2. BASE PROSPECTOS/BASE GENERAL/Base Alonso Huaman.xlsx",
            "table_name": "Base_Alonso",
            "rango_filas": "1:10000"
        },
        {
            "path": "Documentos compartidos/2. BASE PROSPECTOS/BASE GENERAL/Base Diana Chavez.xlsx",
            "table_name": "Base_Diana", 
            "rango_filas": "10001:20000"
        },
        {
            "path": "Documentos compartidos/2. BASE PROSPECTOS/BASE GENERAL/Base Gerson Falen.xlsx",
            "table_name": "Base_Gerson",
            "rango_filas": "20001:30000"
        },
        # ... AGREGA LAS 10 VENDEDORAS
    ]
    
    # Cadena de conexi√≥n Azure SQL
    connection_string = f"""
    Driver={{ODBC Driver 18 for SQL Server}};
    Server={SQL_SERVER};
    Database={SQL_DATABASE};
    Uid={SQL_USERNAME};
    Pwd={SQL_PASSWORD};
    Encrypt=yes;
    TrustServerCertificate=no;
    Connection Timeout=60;
    """
    
    try:
        # Conectar a Azure SQL con reintentos
        conn = connect_sql_with_retry(connection_string)
        cursor = conn.cursor()
        
        # OPTIMIZACI√ìN: Crear tabla temporal para bulk insert
        cursor.execute("""
            IF OBJECT_ID('tempdb..#TempVendedorasData') IS NOT NULL
                DROP TABLE #TempVendedorasData
            
            CREATE TABLE #TempVendedorasData (
                ID INT,
                Ejecutivo NVARCHAR(100),
                Telefono NVARCHAR(50),
                FechaCreada DATETIME,
                Sede NVARCHAR(100),
                Programa NVARCHAR(100),
                Turno NVARCHAR(50),
                Codigo NVARCHAR(50),
                Canal NVARCHAR(100),
                Intervalo NVARCHAR(50),
                Medio NVARCHAR(100),
                Contacto NVARCHAR(100),
                Interesado NVARCHAR(100),
                Estado NVARCHAR(100),
                Objecion NVARCHAR(500),
                Observacion NVARCHAR(1000)
            )
        """)
        
        # Procesar cada vendedora
        total_registros = 0
        for config in VENDEDORAS_CONFIG:
            try:
                logging.info(f"üîÑ Procesando: {config['table_name']}")
                
                # SOLUCI√ìN H√çBRIDA: Links p√∫blicos + autenticaci√≥n
                file_content = download_sharepoint_file_public_with_auth(
                    config['path'], 
                    SHAREPOINT_USERNAME, 
                    SHAREPOINT_PASSWORD
                )
                
                if file_content is None:
                    logging.error(f"‚ùå No se pudo descargar: {config['path']}")
                    continue
                
                # Buscar la tabla en el Excel
                df = find_table_in_excel_optimized(file_content, config['table_name'])
                
                if df is not None and not df.empty:
                    # Limitar a 10,000 filas m√°ximo
                    df = df.head(10000)
                    
                    # OPTIMIZACI√ìN: Insertar en tabla temporal
                    registros_procesados = insert_to_temp_table(cursor, df, config['rango_filas'])
                    total_registros += registros_procesados
                    
                    logging.info(f"‚úÖ {config['table_name']}: {registros_procesados} registros preparados")
                else:
                    logging.error(f"‚ùå No se encontr√≥ tabla v√°lida: {config['table_name']}")
                
            except Exception as e:
                logging.error(f"‚ùå Error procesando {config['table_name']}: {str(e)}")
                continue
        
        # OPTIMIZACI√ìN: Actualizaci√≥n masiva desde tabla temporal
        if total_registros > 0:
            logging.info(f"üîÑ Realizando actualizaci√≥n masiva de {total_registros} registros...")
            actualizacion_masiva(cursor)
        
        # Confirmar todos los cambios
        conn.commit()
        conn.close()
        logging.info(f"üéâ ACTUALIZACI√ìN COMPLETADA - {total_registros} filas actualizadas")
            
    except Exception as e:
        logging.error(f"üí• Error general: {str(e)}")
        raise e

def download_sharepoint_file_public_with_auth(file_path, username, password):
    """SOLUCI√ìN H√çBRIDA OPTIMIZADA: Links p√∫blicos + autenticaci√≥n"""
    
    # LINKS P√öBLICOS √öNICOS
    public_links = {
        "Base Alonso Huaman.xlsx": "https://escuelarefrigeracion.sharepoint.com/:x:/s/ASESORASCOMERCIALES/EaIlXhIcpYBFkaxzXu7aQIQBAu_zaldlNLgtz7y6bOMyCA?e=yVU2iw",
        "Base Diana Chavez.xlsx": "https://escuelarefrigeracion.sharepoint.com/:x:/s/ASESORASCOMERCIALES/EeRBRnXXABpPhWkYk87UcjoB-VltTBFz6MRSQ-VEbucP8Q?e=bvUv7V",
        "Base Gerson Falen.xlsx": "https://escuelarefrigeracion.sharepoint.com/:x:/s/ASESORASCOMERCIALES/EQGtk5_fCslJowZlY8g7kTEBLfD29swdE4DK_0nDfBZ7qw?e=36cm8P"
    }
    
    filename = file_path.split('/')[-1]
    
    if filename in public_links:
        try:
            logging.info(f"üîó Descargando link p√∫blico CON AUTENTICACI√ìN: {filename}")
            
            session = requests.Session()
            session.auth = (username, password)  # ‚¨ÖÔ∏è CLAVE: Agregar autenticaci√≥n
            
            # Headers optimizados
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
                'Accept': '*/*',
                'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8'
            }
            
            # OPTIMIZACI√ìN: Timeout m√°s agresivo y seguimiento de redirecciones
            response = session.get(
                public_links[filename], 
                headers=headers, 
                timeout=30,  # ‚¨ÖÔ∏è Reducido de 60 a 30 segundos
                verify=False,
                allow_redirects=True  # ‚¨ÖÔ∏è IMPORTANTE: Seguir redirecciones
            )
            
            logging.info(f"üìä Response final: HTTP {response.status_code}, Size: {len(response.content)} bytes")
            
            if response.status_code == 200:
                content = response.content
                
                # Verificaci√≥n robusta de contenido Excel
                if len(content) > 1000:
                    content_start = content[:500].decode('utf-8', errors='ignore')
                    
                    # Si es HTML, fall√≥ la autenticaci√≥n
                    if any(keyword in content_start.lower() for keyword in ['<!doctype', '<html', 'login', 'redirect', 'microsoft']):
                        logging.error("‚ùå Autenticaci√≥n fall√≥ - SharePoint devolvi√≥ p√°gina HTML")
                        logging.debug(f"Contenido inicial: {content_start[:200]}")
                        return None
                    
                    # Verificar firma de archivo Excel
                    if (content[:4] == b'PK\x03\x04' or  # Firma ZIP de Office
                        b'[Content_Types]' in content[:2000] or 
                        b'xl/' in content[:1000]):
                        logging.info(f"‚úÖ √âXITO: Excel v√°lido detectado - {len(content)} bytes")
                        return BytesIO(content)
                    else:
                        # Intentar procesar de todos modos
                        logging.warning(f"‚ö†Ô∏è Firma Excel no est√°ndar, intentando procesar...")
                        return BytesIO(content)
                else:
                    logging.error(f"‚ùå Archivo demasiado peque√±o: {len(content)} bytes")
                    return None
            else:
                logging.error(f"‚ùå Error HTTP {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            logging.error("‚ùå Timeout en descarga de SharePoint")
            return None
        except Exception as e:
            logging.error(f"‚ùå Error en descarga con autenticaci√≥n: {str(e)}")
            return None
    else:
        logging.warning(f"‚ö†Ô∏è No hay link p√∫blico configurado para: {filename}")
        return None

def find_table_in_excel_optimized(file_content, table_name):
    """B√∫squeda optimizada de tabla en Excel"""
    try:
        # OPTIMIZACI√ìN: Probar solo openpyxl (m√°s r√°pido para .xlsx)
        try:
            excel_file = pd.ExcelFile(file_content, engine='openpyxl')
            
            # Estrategia 1: Buscar en primera hoja (caso m√°s com√∫n)
            try:
                df = pd.read_excel(file_content, sheet_name=0, engine='openpyxl')
                if not df.empty and len(df.columns) > 1:
                    logging.info("‚úÖ Datos encontrados en primera hoja")
                    return clean_dataframe(df)
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Error en primera hoja: {str(e)}")
                pass
            
            # Estrategia 2: Buscar en todas las hojas
            for sheet_name in excel_file.sheet_names:
                try:
                    df = pd.read_excel(file_content, sheet_name=sheet_name, engine='openpyxl')
                    if not df.empty and len(df.columns) > 1:
                        logging.info(f"‚úÖ Datos encontrados en hoja: {sheet_name}")
                        return clean_dataframe(df)
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Error en hoja {sheet_name}: {str(e)}")
                    continue
                    
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Openpyxl fall√≥: {str(e)}")
            # Fallback a xlrd si es necesario
            try:
                df = pd.read_excel(file_content, engine='xlrd')
                return clean_dataframe(df)
            except:
                pass
                
        logging.error("‚ùå No se pudo leer el archivo con ning√∫n engine")
        return None
        
    except Exception as e:
        logging.error(f"‚ùå Error buscando tabla: {str(e)}")
        return None

def clean_dataframe(df):
    """Limpieza y normalizaci√≥n del DataFrame"""
    # Eliminar filas completamente vac√≠as
    df = df.dropna(how='all')
    
    # Normalizar nombres de columnas
    df.columns = [str(col).strip().replace('\n', ' ').replace('\r', '') for col in df.columns]
    
    # Eliminar columnas completamente vac√≠as
    df = df.dropna(axis=1, how='all')
    
    return df

def insert_to_temp_table(cursor, df, rango_filas):
    """Inserci√≥n optimizada en tabla temporal"""
    start_id, end_id = map(int, rango_filas.split(':'))
    
    # Normalizar nombres de columnas
    df.columns = [str(col).strip() for col in df.columns]
    
    # Mapeo optimizado de columnas
    mapeo_columnas = {}
    columnas_requeridas = ['Ejecutivo', 'Telefono', 'FechaCreada', 'Sede', 'Programa', 'Turno', 
                          'Codigo', 'Canal', 'Intervalo', 'Medio', 'Contacto', 'Interesado', 
                          'Estado', 'Objecion', 'Observacion']
    
    for col_requerida in columnas_requeridas:
        for col_real in df.columns:
            if col_requerida.lower() in col_real.lower():
                mapeo_columnas[col_requerida] = col_real
                break
    
    registros_insertados = 0
    batch_data = []
    
    for index, row in df.iterrows():
        current_id = start_id + index
        
        if current_id > end_id:
            break
        
        try:
            # Obtener valores usando mapeo
            valores = [current_id]  # ID primero
            
            for col_requerida in columnas_requeridas:
                col_real = mapeo_columnas.get(col_requerida, col_requerida)
                valor = row.get(col_real, '')
                
                # Manejo optimizado de fechas
                if col_requerida == 'FechaCreada' and pd.notna(valor):
                    try:
                        if isinstance(valor, str):
                            valor = pd.to_datetime(valor, errors='coerce')
                        if pd.notna(valor):
                            valor = valor.strftime('%Y-%m-%d %H:%M:%S')
                        else:
                            valor = None
                    except:
                        valor = None
                elif pd.isna(valor):
                    valor = None
                
                valores.append(valor)
            
            batch_data.append(valores)
            registros_insertados += 1
            
            # OPTIMIZACI√ìN: Inserci√≥n por lotes cada 100 registros
            if len(batch_data) >= 100:
                insert_batch(cursor, batch_data)
                batch_data = []
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Error procesando fila {index}: {str(e)}")
            continue
    
    # Insertar lote final
    if batch_data:
        insert_batch(cursor, batch_data)
    
    return registros_insertados

def insert_batch(cursor, batch_data):
    """Inserci√≥n por lotes optimizada"""
    try:
        placeholders = ','.join(['?'] * 16)  # 16 columnas
        sql = f"INSERT INTO #TempVendedorasData VALUES ({placeholders})"
        cursor.executemany(sql, batch_data)
    except Exception as e:
        logging.error(f"‚ùå Error en inserci√≥n por lote: {str(e)}")

def actualizacion_masiva(cursor):
    """Actualizaci√≥n masiva desde tabla temporal"""
    try:
        # OPTIMIZACI√ìN: Single UPDATE con JOIN
        cursor.execute("""
            UPDATE v 
            SET 
                v.Ejecutivo = t.Ejecutivo,
                v.Telefono = t.Telefono,
                v.FechaCreada = t.FechaCreada,
                v.Sede = t.Sede,
                v.Programa = t.Programa,
                v.Turno = t.Turno,
                v.Codigo = t.Codigo,
                v.Canal = t.Canal,
                v.Intervalo = t.Intervalo,
                v.Medio = t.Medio,
                v.Contacto = t.Contacto,
                v.Interesado = t.Interesado,
                v.Estado = t.Estado,
                v.Objecion = t.Objecion,
                v.Observacion = t.Observacion
            FROM vendedoras_data v
            INNER JOIN #TempVendedorasData t ON v.ID = t.ID
        """)
        
        filas_afectadas = cursor.rowcount
        logging.info(f"üìä Actualizaci√≥n masiva completada: {filas_afectadas} filas afectadas")
        
    except Exception as e:
        logging.error(f"‚ùå Error en actualizaci√≥n masiva: {str(e)}")
        raise

def connect_sql_with_retry(connection_string, max_retries=3):
    """Conectar a SQL con reintentos optimizado"""
    for attempt in range(max_retries):
        try:
            conn = pyodbc.connect(connection_string)
            logging.info(f"‚úÖ Conexi√≥n SQL exitosa (intento {attempt + 1})")
            return conn
        except pyodbc.OperationalError as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5  # ‚¨ÖÔ∏è Reducido de 10 a 5 segundos
                logging.warning(f"‚ö†Ô∏è Intento {attempt + 1} fallado, reintentando en {wait_time}s: {str(e)}")
                time.sleep(wait_time)
            else:
                logging.error(f"üí• Todos los intentos fallaron: {str(e)}")
                raise e

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('sync_sharepoint.log')
        ]
    )
    
    start_time = time.time()
    sync_sharepoint_to_sql()
    end_time = time.time()
    
    logging.info(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {end_time - start_time:.2f} segundos")
